{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe==0.10.11 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.10.11)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mediapipe==0.10.11) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mediapipe==0.10.11) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mediapipe==0.10.11) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mediapipe==0.10.11) (0.4.34)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mediapipe==0.10.11) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mediapipe==0.10.11) (1.26.3)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mediapipe==0.10.11) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mediapipe==0.10.11) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mediapipe==0.10.11) (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sounddevice>=0.4.4->mediapipe==0.10.11) (1.17.1)\n",
      "Requirement already satisfied: jaxlib<=0.4.34,>=0.4.34 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jax->mediapipe==0.10.11) (0.4.34)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jax->mediapipe==0.10.11) (0.5.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jax->mediapipe==0.10.11) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jax->mediapipe==0.10.11) (1.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->mediapipe==0.10.11) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->mediapipe==0.10.11) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->mediapipe==0.10.11) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->mediapipe==0.10.11) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->mediapipe==0.10.11) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->mediapipe==0.10.11) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->mediapipe==0.10.11) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->mediapipe==0.10.11) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.11) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.11) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe==0.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (8.3.11)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (1.26.3)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (2.4.1+cu124)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (0.19.1+cu124)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (2.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import Image, clear_output\n",
    "from torch.cuda import memory_allocated, empty_cache\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\project\\New_Data\\Image\n",
      "67840\n",
      "67840 67840\n"
     ]
    }
   ],
   "source": [
    "data_root = r'E:\\project\\New_Data'\n",
    "file_root = rf'{data_root}\\Image'\n",
    "project_name = 'fall_detection'\n",
    "print(file_root)\n",
    "train_root = rf'{data_root}\\train'\n",
    "valid_root = rf'{data_root}\\valid'\n",
    "test_root = rf'{data_root}\\test'\n",
    "cls_list = ['Normal', 'Fall']\n",
    "cls_filename_list = ['N', 'BY', 'FY', 'SY']\n",
    "\n",
    "image_root = f'{train_root}\\\\images'\n",
    "label_root = f'{train_root}\\\\labels'\n",
    "\n",
    "image_root_4k = r'E:\\project\\Data\\Data\\Data\\Training\\SourceData\\TS\\Image'\n",
    "\n",
    "image_list = sorted(glob.glob(f'{image_root}\\\\*.jpg'))\n",
    "print(len(image_list))\n",
    "label_list = [x.replace('images', 'labels').replace('JPG', 'jpg').replace('jpg', 'txt') for x in image_list]\n",
    "print(len(image_list), len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old\n",
    "def image_sampling(image_root, label_root, sample_num):\n",
    "    image_cls_dict = {'N': [], 'BY': [], 'FY': [], 'SY': []}\n",
    "    image_names_dict = {'N': [], 'BY': [], 'FY': [], 'SY': []}\n",
    "    image_sampled_dict = {'N': [], 'BY': [], 'FY': [], 'SY': []}\n",
    "    label_sampled_dict = {'N': [], 'BY': [], 'FY': [], 'SY': []}\n",
    "    for cls in cls_filename_list:\n",
    "        image_cls_dict[cls] = glob.glob(f'{image_root}\\\\*_{cls}_*')\n",
    "        image_names_dict[cls] = list(set([x.split('\\\\')[-1].split('_')[0] for x in image_cls_dict[cls]]))\n",
    "        div = 2 if cls == 'N' else 6\n",
    "        image_names_dict[cls] = random.sample(image_names_dict[cls], round((sample_num / 80) / div))\n",
    "        for i in tqdm(image_names_dict[cls]):\n",
    "            image_sampled_dict[cls].extend(sorted(glob.glob(f'{image_root}\\\\{i}_*')))\n",
    "            label_sampled_dict[cls].extend(sorted(glob.glob(f'{label_root}\\\\{i}_*')))\n",
    "    print([image_sampled_dict[cls][:10] for cls in cls_filename_list])\n",
    "    print([label_sampled_dict[cls][:10] for cls in cls_filename_list])\n",
    "    print([len(image_sampled_dict[cls]) for cls in cls_filename_list])\n",
    "    return image_sampled_dict, label_sampled_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_sampling_4k(image_root, sample_num):\n",
    "    image_cls_dict = {'N': [], 'BY': [], 'FY': [], 'SY': []}\n",
    "    image_names_dict = {'N': [], 'BY': [], 'FY': [], 'SY': []}\n",
    "    image_sampled_dict = {'N': [], 'BY': [], 'FY': [], 'SY': []}\n",
    "    for cls in cls_filename_list:\n",
    "        image_cls_dict[cls] = glob.glob(f'{image_root}\\\\*\\\\{cls}\\\\*')\n",
    "        image_names_dict[cls] = list(set([x.split('\\\\')[-1].split('_')[0] for x in image_cls_dict[cls]]))\n",
    "        div = 2 if cls == 'N' else 6\n",
    "        image_names_dict[cls] = random.sample(image_names_dict[cls], round((sample_num / 80) / div))\n",
    "        for i in tqdm(image_names_dict[cls]):\n",
    "            image_sampled_dict[cls].extend(sorted(glob.glob(f'{image_root_4k}\\\\*\\\\{cls}\\\\{i}_*\\\\*')))\n",
    "        print([image_sampled_dict[cls][:10] for cls in cls_filename_list])\n",
    "        print([len(image_sampled_dict[cls]) for cls in cls_filename_list])\n",
    "    return image_sampled_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [01:02<00:00,  3.02it/s]\n",
      "100%|██████████| 62/62 [00:20<00:00,  3.02it/s]\n",
      "100%|██████████| 62/62 [00:20<00:00,  3.00it/s]\n",
      "100%|██████████| 62/62 [00:20<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['E:\\\\project\\\\New_Data\\\\train\\\\images\\\\00873_O_E_N_C1_I001.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\00873_O_E_N_C1_I002.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\00873_O_E_N_C1_I003.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\00873_O_E_N_C1_I004.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\00873_O_E_N_C1_I005.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\00873_O_E_N_C1_I006.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\00873_O_E_N_C1_I007.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\00873_O_E_N_C1_I008.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\00873_O_E_N_C1_I009.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\00873_O_E_N_C1_I010.jpg'], ['E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01346_O_F_BY_C1_I001.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01346_O_F_BY_C1_I002.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01346_O_F_BY_C1_I003.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01346_O_F_BY_C1_I004.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01346_O_F_BY_C1_I005.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01346_O_F_BY_C1_I006.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01346_O_F_BY_C1_I007.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01346_O_F_BY_C1_I008.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01346_O_F_BY_C1_I009.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01346_O_F_BY_C1_I010.jpg'], ['E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01696_Y_E_FY_C1_I001.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01696_Y_E_FY_C1_I002.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01696_Y_E_FY_C1_I003.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01696_Y_E_FY_C1_I004.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01696_Y_E_FY_C1_I005.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01696_Y_E_FY_C1_I006.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01696_Y_E_FY_C1_I007.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01696_Y_E_FY_C1_I008.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01696_Y_E_FY_C1_I009.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\01696_Y_E_FY_C1_I010.jpg'], ['E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02032_H_A_SY_C1_I001.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02032_H_A_SY_C1_I002.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02032_H_A_SY_C1_I003.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02032_H_A_SY_C1_I004.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02032_H_A_SY_C1_I005.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02032_H_A_SY_C1_I006.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02032_H_A_SY_C1_I007.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02032_H_A_SY_C1_I008.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02032_H_A_SY_C1_I009.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02032_H_A_SY_C1_I010.jpg']]\n",
      "[['E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\00873_O_E_N_C1_I001.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\00873_O_E_N_C1_I002.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\00873_O_E_N_C1_I003.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\00873_O_E_N_C1_I004.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\00873_O_E_N_C1_I005.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\00873_O_E_N_C1_I006.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\00873_O_E_N_C1_I007.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\00873_O_E_N_C1_I008.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\00873_O_E_N_C1_I009.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\00873_O_E_N_C1_I010.txt'], ['E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01346_O_F_BY_C1_I001.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01346_O_F_BY_C1_I002.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01346_O_F_BY_C1_I003.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01346_O_F_BY_C1_I004.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01346_O_F_BY_C1_I005.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01346_O_F_BY_C1_I006.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01346_O_F_BY_C1_I007.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01346_O_F_BY_C1_I008.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01346_O_F_BY_C1_I009.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01346_O_F_BY_C1_I010.txt'], ['E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01696_Y_E_FY_C1_I001.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01696_Y_E_FY_C1_I002.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01696_Y_E_FY_C1_I003.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01696_Y_E_FY_C1_I004.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01696_Y_E_FY_C1_I005.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01696_Y_E_FY_C1_I006.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01696_Y_E_FY_C1_I007.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01696_Y_E_FY_C1_I008.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01696_Y_E_FY_C1_I009.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\01696_Y_E_FY_C1_I010.txt'], ['E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\02032_H_A_SY_C1_I001.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\02032_H_A_SY_C1_I002.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\02032_H_A_SY_C1_I003.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\02032_H_A_SY_C1_I004.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\02032_H_A_SY_C1_I005.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\02032_H_A_SY_C1_I006.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\02032_H_A_SY_C1_I007.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\02032_H_A_SY_C1_I008.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\02032_H_A_SY_C1_I009.txt', 'E:\\\\project\\\\New_Data\\\\train\\\\labels\\\\02032_H_A_SY_C1_I010.txt']]\n",
      "[15040, 4960, 4960, 4960]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15040"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원하는 양의 이미지&라벨을 샘플링한 후 각 클래스(N, BY, SY, FY) 형태로 저장\n",
    "image_dict, label_dict = image_sampling(image_root, label_root, 30000)\n",
    "len(label_dict['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:02<00:00, 93.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I001.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I002.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I003.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I004.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I005.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I006.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I007.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I008.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I009.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I010.jpg'], [], [], []]\n",
      "[15040, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 89.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I001.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I002.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I003.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I004.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I005.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I006.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I007.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I008.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I009.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I010.jpg'], ['E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I001.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I002.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I003.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I004.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I005.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I006.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I007.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I008.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I009.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I010.jpg'], [], []]\n",
      "[15040, 4960, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 73.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I001.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I002.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I003.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I004.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I005.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I006.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I007.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I008.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I009.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I010.jpg'], ['E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I001.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I002.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I003.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I004.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I005.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I006.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I007.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I008.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I009.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I010.jpg'], ['E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I001.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I002.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I003.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I004.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I005.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I006.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I007.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I008.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I009.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I010.jpg'], []]\n",
      "[15040, 4960, 4960, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 122.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I001.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I002.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I003.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I004.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I005.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I006.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I007.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I008.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I009.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\N\\\\N\\\\01136_O_E_N_C1\\\\01136_O_E_N_C1_I010.jpg'], ['E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I001.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I002.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I003.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I004.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I005.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I006.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I007.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I008.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I009.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\BY\\\\01788_Y_E_BY_C1\\\\01788_Y_E_BY_C1_I010.jpg'], ['E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I001.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I002.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I003.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I004.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I005.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I006.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I007.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I008.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I009.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\FY\\\\00044_H_A_FY_C1\\\\00044_H_A_FY_C1_I010.jpg'], ['E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\SY\\\\01735_Y_E_SY_C1\\\\01735_Y_E_SY_C1_I001.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\SY\\\\01735_Y_E_SY_C1\\\\01735_Y_E_SY_C1_I002.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\SY\\\\01735_Y_E_SY_C1\\\\01735_Y_E_SY_C1_I003.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\SY\\\\01735_Y_E_SY_C1\\\\01735_Y_E_SY_C1_I004.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\SY\\\\01735_Y_E_SY_C1\\\\01735_Y_E_SY_C1_I005.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\SY\\\\01735_Y_E_SY_C1\\\\01735_Y_E_SY_C1_I006.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\SY\\\\01735_Y_E_SY_C1\\\\01735_Y_E_SY_C1_I007.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\SY\\\\01735_Y_E_SY_C1\\\\01735_Y_E_SY_C1_I008.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\SY\\\\01735_Y_E_SY_C1\\\\01735_Y_E_SY_C1_I009.jpg', 'E:\\\\project\\\\Data\\\\Data\\\\Data\\\\Training\\\\SourceData\\\\TS\\\\Image\\\\Y\\\\SY\\\\01735_Y_E_SY_C1\\\\01735_Y_E_SY_C1_I010.jpg']]\n",
      "[15040, 4960, 4960, 4960]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15040"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원하는 양의 이미지&라벨을 샘플링한 후 각 클래스(N, BY, SY, FY) 형태로 저장 (4k 원본 이미지)\n",
    "image_dict_4k = image_sampling_4k(image_root_4k, 30000)\n",
    "len(image_dict_4k['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCH = 2\n",
    "NUM_LAYERS = 1\n",
    "n_CONFIDENCE = 0.3\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "# pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "yolo_model = YOLO(r'E:\\project\\code\\Project_humanFall\\runs\\detect\\human_fall_s30\\weights\\best.pt', verbose=False)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "attention_dot = [n for n in range(33)]\n",
    "\n",
    "\n",
    "draw_line_list = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], [12, 14], [14, 16],\n",
    "             [16, 22], [16, 20], [16, 18], [18, 20], [23, 25], [25, 27], [24, 26], [26, 28],\n",
    "             [11, 12], [11, 23], [23, 24], [12, 24], [9, 10], [0, 5], [0, 2], [5, 8], [2, 7],\n",
    "             [28, 30], [30, 32], [28, 32], [27, 29], [29, 31], [27, 31]]\n",
    "COLOR_PINK = (255, 51, 153)\n",
    "COLOR_WHITE = (255, 255, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_imshow_samples(\n",
    "        draw_line_list,\n",
    "        draw_line_dic,\n",
    "        img_ori,\n",
    "        img_pose,\n",
    "        bx1, by1, bx2, by2,\n",
    "        img_src, isFailed\n",
    "        ):\n",
    "    img_ori = cv2.resize(img_ori, (960, 540))\n",
    "    cv2.putText(img_ori, img_src.split('\\\\')[-1], (5, 30), cv2.FONT_HERSHEY_DUPLEX, 1, COLOR_PINK, lineType=cv2.LINE_AA)\n",
    "    if isFailed == 'Detected':\n",
    "        for line in draw_line_list:\n",
    "            lx1, ly1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "            lx2, ly2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "            img_pose = cv2.line(img_pose, (bx1 + lx1, by1 + ly1), (bx1 + lx2, by1 + ly2), (0, 255, 0), 5)\n",
    "            \n",
    "        img_pose = cv2.rectangle(img_pose, (bx1, by1), (bx2, by2), (0, 255, 0), 2)\n",
    "        img_pose = cv2.resize(img_pose, (960, 540))\n",
    "        return [img_ori, img_pose, isFailed, 0]\n",
    "    elif isFailed == 'Failed!: Pose Out of BBOX':\n",
    "        for line in draw_line_list:\n",
    "            lx1, ly1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "            lx2, ly2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "            img_pose = cv2.line(img_pose, (bx1 + lx1, by1 + ly1), (bx1 + lx2, by1 + ly2), (0, 255, 0), 5)\n",
    "            \n",
    "        img_pose = cv2.rectangle(img_pose, (bx1, by1), (bx2, by2), (0, 255, 0), 2)\n",
    "        img_pose = cv2.resize(img_pose, (960, 540))\n",
    "        return [img_ori, False, isFailed, img_pose]\n",
    "    else:\n",
    "        return [img_ori, False, isFailed, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose(image_list: list, label_list: list, attention_dot: list, draw_line_list: list, isSample=False, sample_size=0):\n",
    "    if isSample:\n",
    "        print('sample mod')\n",
    "        print(len(image_list))\n",
    "        plt.figure(figsize=(8, 4*sample_size))\n",
    "    frame_length = 10 # LSTM 모델에 넣을 frame 수\n",
    "    xy_list_list = []\n",
    "    omission_cnt = 0\n",
    "    for i, (img_src, lbl) in enumerate((zip(image_list, label_list))):\n",
    "        if label_type == 'yolo':\n",
    "            with open(lbl, 'r', encoding='utf-8') as f:\n",
    "                label_str = f.read()\n",
    "                bbox = list(map(float, label_str.split(' ')[1:]))\n",
    "\n",
    "            bbox = [int(x * 640) for x in bbox]\n",
    "            bbox = [\n",
    "                max(int(bbox[0] - bbox[2] / 2 - 30), 0),\n",
    "                max(int(bbox[1] - bbox[3] / 2 - 30), 0),\n",
    "                min(int(bbox[0] + bbox[2] / 2 + 30), 640),\n",
    "                min(int(bbox[1] + bbox[3] / 2 + 30), 640),\n",
    "            ]\n",
    "            img = cv2.imread(img_src)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if isSample:\n",
    "                bbox_img = cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 0, 0), 2)\n",
    "                cv2.putText(bbox_img, img_src.split('\\\\')[-1], (5, 30), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), lineType=cv2.LINE_AA)\n",
    "                plt.subplot(sample_size, 2, (2*i+1))\n",
    "                plt.imshow(bbox_img)\n",
    "\n",
    "        \n",
    "        \"\"\" Yolov5 바운딩 박스 좌표 안에서 mediapipe Pose 추출\"\"\"\n",
    "\n",
    "        c_img = img[bbox[1]:bbox[3], bbox[0]:bbox[2]] # 바운딩 박스 좌표/\n",
    "        del img\n",
    "        results = pose.process(c_img) # Yolov5 바운딩 박스 좌표 안에서 'mp_pose' 좌표\n",
    "\n",
    "        if not results.pose_landmarks: continue\n",
    "        idx = 0\n",
    "        draw_line_dic = {}\n",
    "        xy_list = []\n",
    "        # 33 반복문 진행 : 33개 중 18개의 dot\n",
    "        for x_and_y in results.pose_landmarks.landmark:\n",
    "            if idx in attention_dot:\n",
    "                xy_list.append(x_and_y.x)\n",
    "                xy_list.append(x_and_y.y)\n",
    "\n",
    "                x, y = int(x_and_y.x * (bbox[2] - bbox[0])), int(x_and_y.y * (bbox[3] - bbox[1]))\n",
    "                draw_line_dic[idx] = [x, y]\n",
    "            idx += 1\n",
    "\n",
    "        if len(xy_list) != len(attention_dot) * 2:\n",
    "            print('Error : attention_dot 데이터 오류')\n",
    "\n",
    "        xy_list_list.append(xy_list)\n",
    "\n",
    "        \"\"\"mediapipe line 그리기 부분 : 데이터 추출(dot) 확인용\"\"\"\n",
    "        if isSample:\n",
    "            for line in draw_line_list:\n",
    "                x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "                x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "                c_img = cv2.line(c_img, (x1, y1), (x2, y2), (0, 255, 0), 4)\n",
    "            plt.subplot(sample_size, 2, (2*i+2))\n",
    "            plt.imshow(c_img)\n",
    "        del c_img\n",
    "        \n",
    "    # 부족한 프레임 수 맞추기\n",
    "    if isSample:\n",
    "        if len(xy_list_list) < len(image_list) // 2:\n",
    "            return False\n",
    "        elif len(xy_list_list) < len(image_list):\n",
    "            f_ln = len(image_list) - len(xy_list)\n",
    "            for _ in range(f_ln):\n",
    "                xy_list_list.append(xy_list_list[-1])\n",
    "    else:\n",
    "        if len(xy_list_list) < 5:\n",
    "            return False, 5 - len(xy_list_list)\n",
    "        elif len(xy_list_list) < 10:\n",
    "            f_ln = frame_length - len(xy_list)\n",
    "            for _ in range(f_ln):\n",
    "                xy_list_list.append(xy_list_list[-1])\n",
    "                omission_cnt = 1\n",
    "    \n",
    "    if isSample:\n",
    "        plt.show()\n",
    "    return xy_list_list, omission_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_yolo(image_list: list, label_type: str, attention_dot: list, draw_line_list: list, isSample=False, sample_size=0):\n",
    "    if isSample:\n",
    "        print('sample mod')\n",
    "        print(len(image_list))\n",
    "        plt.figure(figsize=(8, 4*sample_size))\n",
    "    frame_length = 10 # LSTM 모델에 넣을 frame 수\n",
    "    xy_list_list = []\n",
    "    omission_cnt = 0\n",
    "    landmarks_prior = []\n",
    "    for i, img_src in enumerate(image_list):\n",
    "        img = cv2.imread(img_src)\n",
    "        if label_type == 'yolo':\n",
    "            results = yolo_model(img, verbose=False)\n",
    "            for r in results:\n",
    "                boxes = r.boxes\n",
    "                for box in boxes:\n",
    "                    # 사람 클래스인 경우에만 처리 (YOLO 클래스 인덱스에 따라 조정 필요)\n",
    "                    # if int(box.cls) == 0:  # 0은 보통 'person' 클래스\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    bx1, by1, bx2, by2 = (\n",
    "                        max(int(x1 - (x2 - x1)*0.2), 0),\n",
    "                        max(int(y1 - (y2 - y1)*0.2), 0),\n",
    "                        min(int(x2 + (x2 - x1)*0.2), 640),\n",
    "                        min(int(y2 + (y2 - y1)*0.2), 640),\n",
    "                    )\n",
    "                    # 바운딩 박스 추출\n",
    "                    person_image = img[by1:by2, bx1:bx2]\n",
    "                    if isSample:\n",
    "                        img_show = img.copy() # cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        cv2.putText(img_show, img_src.split('\\\\')[-1], (5, 30), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), lineType=cv2.LINE_AA)\n",
    "                        # plt.subplot(sample_size, 2, (2*i+1))\n",
    "                        # plt.imshow(img_show)\n",
    "                        cv2.imshow('original_img', img_show)\n",
    "                        cv2.waitKey(1)\n",
    "                        del img_show\n",
    "                    \n",
    "                    # MediaPipe로 포즈 추정\n",
    "                    results_pose = pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
    "                    \n",
    "                    idx = 0\n",
    "                    draw_line_dic = {}\n",
    "                    xy_list = []\n",
    "\n",
    "                    # pose 인식 여부 확인하고 인식 실패 시 이전 이미지로 대체\n",
    "                    if results_pose.pose_landmarks:\n",
    "                        landmarks = results_pose.pose_landmarks\n",
    "                    else:\n",
    "                        if i == 0: continue\n",
    "                        landmarks = landmarks_prior\n",
    "                        img = cv2.putText(img, 'detect failed!', (5, 30), cv2.FONT_HERSHEY_DUPLEX, 0.7, (255, 255, 255), lineType=cv2.LINE_AA)\n",
    "                        omission_cnt += 1\n",
    "\n",
    "                    # pose 좌표값 list 생성\n",
    "                    for x_and_y in landmarks.landmark:\n",
    "                        print(x_and_y)\n",
    "                        if idx in attention_dot:\n",
    "                            xy_list.append(x_and_y.x)\n",
    "                            xy_list.append(x_and_y.y)\n",
    "\n",
    "                            x, y = int(x_and_y.x * (x2 - x1)), int(x_and_y.y * (y2 - y1))\n",
    "                            draw_line_dic[idx] = [x, y]\n",
    "                        idx += 1\n",
    "\n",
    "                    if len(xy_list) != len(attention_dot) * 2:\n",
    "                        print('Error : attention_dot 데이터 오류')\n",
    "\n",
    "                    xy_list_list.append(xy_list)\n",
    "\n",
    "                    # 원본 이미지에 포즈 그리기\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        img[y1:y2, x1:x2],\n",
    "                        landmarks,\n",
    "                        mp_pose.POSE_CONNECTIONS,\n",
    "\n",
    "                    )\n",
    "\n",
    "                    # if isSample:\n",
    "                    #     for line in draw_line_list:\n",
    "                    #         lx1, ly1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "                    #         lx2, ly2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "                    #         img = cv2.line(img, (x1 + lx1, y1 + ly1), (x1 + lx2, y1 + ly2), (0, 255, 0), 2)\n",
    "                    landmarks_prior = landmarks\n",
    "                        \n",
    "                    if isSample:\n",
    "                        img_show = img.copy()\n",
    "                        img_show = cv2.rectangle(img_show, (bx1, by1), (bx2, by2), (0, 255, 0), 2)\n",
    "                        # plt.subplot(sample_size, 2, (2*i+2))\n",
    "                        # plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "                        cv2.imshow('img', img_show)\n",
    "                        cv2.waitKey()\n",
    "                        del img_show\n",
    "            \n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "    # 부족한 프레임 수 맞추기\n",
    "    # if isSample:\n",
    "    #     if len(xy_list_list) < len(image_list) // 2:\n",
    "    #         return False\n",
    "    #     elif len(xy_list_list) < len(image_list):\n",
    "    #         f_ln = len(image_list) - len(xy_list)\n",
    "    #         for _ in range(f_ln):\n",
    "    #             xy_list_list.append(xy_list_list[-1])\n",
    "    # else:\n",
    "    #     if len(xy_list_list) < 5:\n",
    "    #         return False, 5 - len(xy_list_list)\n",
    "    #     elif len(xy_list_list) < 10:\n",
    "    #         f_ln = frame_length - len(xy_list)\n",
    "    #         for _ in range(f_ln):\n",
    "    #             xy_list_list.append(xy_list_list[-1])\n",
    "    #             omission_cnt = 1\n",
    "    \n",
    "    # if isSample:\n",
    "    #     plt.show()\n",
    "    return xy_list_list, omission_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_4k(image_list: list, attention_dot: list, draw_line_list: list, isSample=False, sample_size=0):\n",
    "    with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5) as pose:\n",
    "        # if isSample: ==> get_pose_sample_4k 함수를 통해서 호출되었을 시 동작하는 부분 >> cv2.imshow()를 위한 동작 수행\n",
    "        if isSample: \n",
    "            print('sample mod')\n",
    "            print(len(image_list))\n",
    "            imshow_name_list = []\n",
    "            imshow_img_list = []\n",
    "        frame_length = 10 # LSTM 모델에 넣을 frame 수\n",
    "        pose_coor_1case = [] # 한 건(연결된 이미지 10장) 안에서의 pose_coor을 모은 것\n",
    "        omission_cnt = 0\n",
    "        landmarks_prior = []\n",
    "        for i, img_src in enumerate(image_list):\n",
    "            img = cv2.imread(img_src)\n",
    "            if isSample: img_ori = img.copy()\n",
    "            results = yolo_model(img, verbose=False)\n",
    "            for r in results:\n",
    "                boxes = r.boxes\n",
    "                for box in boxes:\n",
    "                    # 사람 클래스인 경우에만 처리 (YOLO 클래스 인덱스에 따라 조정 필요)\n",
    "                    # if int(box.cls) == 0:  # 0은 보통 'person' 클래스\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    bx1, by1, bx2, by2 = (\n",
    "                        max(int(x1 - (x2 - x1)*0.2), 0),\n",
    "                        max(int(y1 - (y2 - y1)*0.2), 0),\n",
    "                        min(int(x2 + (x2 - x1)*0.2), 3840),\n",
    "                        min(int(y2 + (y2 - y1)*0.2), 2160),\n",
    "                    )\n",
    "                    # 바운딩 박스 추출\n",
    "                    person_image = img[by1:by2, bx1:bx2]\n",
    "                    \n",
    "                    # MediaPipe로 포즈 추정\n",
    "                    results_pose = pose.process(cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                    idx = 0 \n",
    "                    draw_line_dic = {} # 샘플 보여주기용 dict\n",
    "                    pose_coor = [] # 한 이미지에서 관절의 xy좌표값의 list ex) [[12, 44], [122, 334], ... , [394, 664]]\n",
    "\n",
    "                    # pose 인식 여부 확인 \n",
    "                    if results_pose.pose_landmarks:\n",
    "                        landmarks = results_pose.pose_landmarks\n",
    "\n",
    "                        # pose 좌표값 list 생성\n",
    "                        for pose_xy in landmarks.landmark:\n",
    "                            if idx in attention_dot:\n",
    "                                pose_coor.append(pose_xy.x)\n",
    "                                pose_coor.append(pose_xy.y)\n",
    "                                x, y = int(pose_xy.x * (bx2 - bx1)), int(pose_xy.y * (by2 - by1))\n",
    "                                draw_line_dic[idx] = [x, y]\n",
    "                            idx += 1\n",
    "\n",
    "                        if len(pose_coor) != len(attention_dot) * 2:\n",
    "                            print('Error : attention_dot 데이터 오류')\n",
    "\n",
    "                        # pose 좌표값중 어느 하나라도 바운딩박스를 초과할 시, 인식 실패로 판정\n",
    "                        if all(list(map(lambda x: 0 < x < 1, pose_coor))) == False:\n",
    "                            pose_coor_1case.append(False)\n",
    "                            if isSample:\n",
    "                                img_pose = img.copy()\n",
    "                                isFailed = 'Failed!: Pose Out of BBOX'\n",
    "                                omission_cnt += 1\n",
    "                                imshow_img_list.append(make_imshow_samples(draw_line_list, draw_line_dic, img_ori, img_pose, bx1, by1, bx2, by2, img_src, isFailed))\n",
    "                                break\n",
    "                        \n",
    "                        # 결과값 list에 추가\n",
    "                        pose_coor_1case.append(pose_coor)\n",
    "\n",
    "                        if isSample:\n",
    "                            img_pose = img.copy()\n",
    "                            isFailed = 'Detected'\n",
    "                            imshow_img_list.append(make_imshow_samples(draw_line_list, draw_line_dic, img_ori, img_pose, bx1, by1, bx2, by2, img_src, isFailed))\n",
    "\n",
    "                    # 인식 실패 시 False로 표시\n",
    "                    else:\n",
    "                        pose_coor_1case.append(False)\n",
    "                        omission_cnt += 1\n",
    "                        img_pose = img.copy()\n",
    "                        if isSample:\n",
    "                            isFailed = 'Failed!: Can\\'t detect by mediapipe'\n",
    "                            imshow_img_list.append(make_imshow_samples(draw_line_list, draw_line_dic, img_ori, img_pose, bx1, by1, bx2, by2, img_src, isFailed))\n",
    "                            \n",
    "                    break # 우리가 사용하는 .pt 파일은 0: Normal, 1: Fall인데 바운딩박스만 잡으면 큰 상관 없으므로 둘 중 하나만 사용 (임시방편으로 만들었습니다. 더 좋은 방법이 있다면...)\n",
    "    \n",
    "    # mediapipe로 인식 실패한 이미지의 pose데이터 채우기\n",
    "    if pose_coor_1case.count(False) >= 5: # 10개 중 5개 이상이 인식 실패했을 시 해당 10장 전체를 실패 처리\n",
    "        print('more than 5 images fail to be detected!')\n",
    "        return False, 1, 1, pose_coor_1case.count(False)\n",
    "    \n",
    "    while pose_coor_1case.count(False) > 0:\n",
    "        om_idx = pose_coor_1case.index(False) # 인식 실패한 이미지의 위치를 저장\n",
    "\n",
    "        if om_idx == (len(pose_coor_1case) - 1): # 맨 뒤의 이미지는 그 앞의 성공한 이미지로 채움\n",
    "            pose_coor_1case[om_idx] = pose_coor_1case[om_idx - 1].copy()\n",
    "            if isSample:\n",
    "                imshow_img_list[om_idx][1] = imshow_img_list[om_idx + 1][1].copy()\n",
    "\n",
    "        else: # 그 외에는 그 뒤의 성공한 이미지로 채움\n",
    "            while pose_coor_1case[om_idx + 1] == False:\n",
    "                om_idx += 1\n",
    "            pose_coor_1case[om_idx] = pose_coor_1case[om_idx + 1].copy()\n",
    "            if isSample:\n",
    "                imshow_img_list[om_idx][1] = imshow_img_list[om_idx + 1][1].copy()\n",
    "\n",
    "    # sample 이미지를 imshow\n",
    "    if isSample:\n",
    "        imshow_img_iter = iter(imshow_img_list)\n",
    "        while True:\n",
    "            try:\n",
    "                imshow_img = imshow_img_iter.__next__()\n",
    "                cv2.putText(imshow_img[1], imshow_img[2], (5, 30), cv2.FONT_HERSHEY_DUPLEX, 1, COLOR_PINK, lineType=cv2.LINE_AA)\n",
    "                cv2.imshow('ori_img', imshow_img[0])\n",
    "                cv2.imshow('img_with_mediapipe_pose', imshow_img[1])\n",
    "                if imshow_img[2] ==  'Failed!: Pose Out of BBOX':\n",
    "                    cv2.imshow('Pose Out of BBOX:', imshow_img[3])\n",
    "                if cv2.waitKey() & 0xFF == ord('q'):\n",
    "                    print(\"사용자에 의해 종료됨\")\n",
    "                    return 'q'\n",
    "            except StopIteration:\n",
    "                break\n",
    "    cv2.destroyAllWindows()\n",
    "    return pose_coor_1case, min(omission_cnt, 1), 0, omission_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터의 일부를 샘플로 잡고, mediapipe로 추출한 pose 데이터를 matplotlib로 plot\n",
    "def get_pose_sample(image_dict: dict, label_dict: dict, attention_dot: list, draw_line_list: list, sample_size: int, sample_start=0):\n",
    "    image_pose_sample = []\n",
    "    label_pose_sample = []\n",
    "    for cls in cls_filename_list:\n",
    "        div = 4\n",
    "        image_pose_sample.extend(image_dict[cls][sample_start:(sample_start+sample_size//div)])\n",
    "        label_pose_sample.extend(label_dict[cls][sample_start:(sample_start+sample_size//div)])\n",
    "    print(len(image_pose_sample), 'asdf')\n",
    "\n",
    "    xy_sample = get_pose_yolo(image_pose_sample, label_pose_sample, label_type, attention_dot, draw_line_list, True, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터의 일부를 샘플로 잡고, mediapipe로 추출한 pose 데이터를 cv2.imshow로 plot\n",
    "def get_pose_sample_4k(image_dict: dict, attention_dot: list, draw_line_list: list, sample_size: int, sample_start=0):\n",
    "    cv2.destroyAllWindows()\n",
    "    image_pose_sample = []\n",
    "    label_pose_sample = []\n",
    "    for cls in cls_filename_list:\n",
    "        div = 4\n",
    "        image_pose_sample.extend(image_dict[cls][sample_start:(sample_start+sample_size//div)])\n",
    "    print(len(image_pose_sample), 'asdf')\n",
    "    for i in range(sample_size//10):\n",
    "        q = get_pose_4k(image_pose_sample[i*10:(i+1)*10], attention_dot, draw_line_list, True, sample_size)\n",
    "        if q == 'q':\n",
    "            cv2.destroyAllWindows()\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_pose_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mget_pose_sample\u001b[49m(image_dict, label_dict, attention_dot, draw_line_list, sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, sample_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_pose_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "get_pose_sample(image_dict, label_dict, attention_dot, draw_line_list, sample_size=40, sample_start=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 asdf\n",
      "sample mod\n",
      "10\n",
      "sample mod\n",
      "10\n",
      "sample mod\n",
      "10\n",
      "sample mod\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "get_pose_sample_4k(image_dict_4k, attention_dot, draw_line_list, sample_size=40, sample_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 1598972.22it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 2654750.07it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 879923.92it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1319938.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15920\n",
      "[['E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C1_I001.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C1_I002.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C1_I003.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C1_I004.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C1_I005.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C1_I006.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C1_I007.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C1_I008.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C1_I009.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C1_I010.jpg'], ['E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C2_I001.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C2_I002.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C2_I003.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C2_I004.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C2_I005.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C2_I006.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C2_I007.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C2_I008.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C2_I009.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C2_I010.jpg'], ['E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C3_I001.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C3_I002.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C3_I003.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C3_I004.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C3_I005.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C3_I006.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C3_I007.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C3_I008.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C3_I009.jpg', 'E:\\\\project\\\\New_Data\\\\train\\\\images\\\\02270_H_A_N_C3_I010.jpg']]\n",
      "1588\n",
      "1588\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# [[C1_I0001, C1_I0002, ...C1_I0010], [C2_I0001, ...], ...] 형태로 묶음\n",
    "def collect_by_case(image_dict, label_dict=0):\n",
    "    raw_data = []\n",
    "    image_pose_input = []\n",
    "    label_pose_input = []\n",
    "    for cls in cls_filename_list:\n",
    "        img_name = {'index': 'None', 'direction': 'None'}\n",
    "        img_bundle = []\n",
    "        label_bundle = []\n",
    "        isNewBundle = 0\n",
    "        dict_iter = zip(tqdm(image_dict[cls]), label_dict[cls]) if label_dict != 0 else tqdm(image_dict[cls])\n",
    "        for img, lbl in dict_iter:\n",
    "            # 처음 값 입력\n",
    "            if img_name['index'] != img.split('\\\\')[-1].split('_')[0]:\n",
    "                img_name['index'] = img.split('\\\\')[-1].split('_')[0]\n",
    "                isNewBundle += 1\n",
    "            if img_name['direction'] != img.split('\\\\')[-1].split('_')[-2]:\n",
    "                img_name['direction'] = img.split('\\\\')[-1].split('_')[-2]\n",
    "                isNewBundle += 1\n",
    "            if isNewBundle > 0:\n",
    "                if img_bundle != []:\n",
    "                    image_pose_input.append(img_bundle)\n",
    "                    if label_dict != 0: label_pose_input.append(label_bundle)\n",
    "                img_bundle = []\n",
    "                label_bundle = []\n",
    "                isNewBundle = 0\n",
    "            img_bundle.append(img)\n",
    "            if label_dict != 0: label_bundle.append(lbl)\n",
    "\n",
    "\n",
    "    print(len(image_dict['N'])+len(image_dict['BY'])+len(image_dict['FY'])+len(image_dict['SY']))\n",
    "    print(image_pose_input[:3])\n",
    "    print(len(image_pose_input))\n",
    "    print(len(label_pose_input))\n",
    "\n",
    "    return image_pose_input, label_pose_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapipe로 pose data를 list로 추출 후, raw_data에 [{낙상, pose data}, {비낙상, pose data}, ...] 형태로 저장\n",
    "\n",
    "# pose = mp_pose.Pose(static_image_mode = True, model_complexity = 1, enable_segmentation = False, min_detection_confidence = n_CONFIDENCE)\n",
    "# ommision_total = 0 # 10장의 이미지가 모두 온전히 인식되지 않은 묶음의 개수\n",
    "# with tqdm(total=len(image_pose_input)) as pbar:\n",
    "#     for img_list, lbl_list in zip(image_pose_input, label_pose_input):\n",
    "#         cls = 0 if img_list[0].split('\\\\')[-1].split('_')[-3] == 'N' else 1\n",
    "#         pose_list, ommision_cnt = get_pose(img_list, lbl_list, 'yolo', attention_dot, draw_line_list, False)\n",
    "#         if pose_list != False:\n",
    "#             raw_data.append({'key': cls, 'value': pose_list})\n",
    "#         pbar.update(1)\n",
    "#         ommision_total += ommision_cnt\n",
    "\n",
    "# print('ommision: ', ommision_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 72/2988 [01:00<40:38,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] <class 'list'>\n",
      "E:\\project\\Data\\Data\\Data\\Training\\SourceData\\TS\\Image\\N\\N\\02073_H_A_N_C1\\02073_H_A_N_C1_I002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "AttributeError: 'list' object has no attribute 'landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 56\u001b[0m, in \u001b[0;36mget_pose_4k\u001b[1;34m(image_list, attention_dot, draw_line, isSample, sample_size)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_and_y \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlandmarks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlandmark\u001b[49m:\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m attention_dot:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'landmark'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_list \u001b[38;5;129;01min\u001b[39;00m image_pose_input:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m img_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 7\u001b[0m     pose_list, ommision_cnt \u001b[38;5;241m=\u001b[39m \u001b[43mget_pose_4k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraw_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pose_list \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m         raw_data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: pose_list})\n",
      "Cell \u001b[1;32mIn[75], line 67\u001b[0m, in \u001b[0;36mget_pose_4k\u001b[1;34m(image_list, attention_dot, draw_line, isSample, sample_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(landmarks, \u001b[38;5;28mtype\u001b[39m(landmarks))\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(img_src)\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttributeError: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandmark\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xy_list) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(attention_dot) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError : attention_dot 데이터 오류\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: AttributeError: 'list' object has no attribute 'landmark'"
     ]
    }
   ],
   "source": [
    "# mediapipe로 pose data를 list로 추출 후, raw_data에 [{낙상, pose data}, {비낙상, pose data}, ...] 형태로 저장 (4k 원본 이미지, yolo 사용)\n",
    "\n",
    "omission_included_cnt = 0 # 10장 중 누락된 이미지가 있는 그룹의 개수\n",
    "totally_omitted_cnt = 0 # 10장 중 5장 이상이 누락되어 사용하지 않는 그룹의 개수\n",
    "omission_total = 0 # 인식되지 않은 이미지의 개수\n",
    "image_pose_input, _ = collect_by_case(image_dict_4k)\n",
    "raw_data = []\n",
    "\n",
    "for img_list in tqdm(image_pose_input):\n",
    "    cls = 0 if img_list[0].split('\\\\')[-1].split('_')[-3] == 'N' else 1\n",
    "    pose_list, isIncludingOmission, isTotallyOmitted, omission_cnt = get_pose_4k(img_list, attention_dot, draw_line_list, False)\n",
    "    if pose_list != False:\n",
    "        raw_data.append({'key': cls, 'value': pose_list})\n",
    "    omission_included_cnt += isIncludingOmission\n",
    "    totally_omitted_cnt += isTotallyOmitted\n",
    "    omission_total += omission_cnt\n",
    "\n",
    "print('omission_included_cnt: ', omission_included_cnt)\n",
    "print('totally_omitted_cnt: ', totally_omitted_cnt)\n",
    "print('ommision_cnt: ', omission_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data)\n",
    "nd = 0\n",
    "ad = 0\n",
    "for i in range(len(raw_data)):\n",
    "    if raw_data[i]['key'] == 0:\n",
    "        nd += 1\n",
    "    else:\n",
    "        ad += 1\n",
    "print('normal data:', nd, '| fall data:', ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, seq_list):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for dic in seq_list :\n",
    "            self.y.append(dic['key'])\n",
    "            self.X.append(dic['value'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = [0.8, 0.1, 0.1]\n",
    "train_len = int(len(raw_data) * split_ratio[0])\n",
    "val_len = int(len(raw_data) * split_ratio[1])\n",
    "test_len = len(raw_data) - train_len - val_len\n",
    "\n",
    "print('{}, {}, {}'.format(train_len, val_len, test_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(raw_data)\n",
    "train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "class skeleton_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(skeleton_LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=len(attention_dot) * 2, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.fc = nn.Linear(32,2)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        x, _ = self.lstm6(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm7(x)\n",
    "        x = self.fc(x[:,-1,:]) # x[배치 크기, 시퀀스 길이, 은닉 상태 크기], [:, -1, :] -> 마지막 시간 단계만 선택\n",
    "\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "\n",
    "def init_model():\n",
    "    global net, loss_fn, optim\n",
    "    plt.rc('font', size = 10)\n",
    "    net = skeleton_LSTM().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optim = Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# epoch 카운터 초기화\n",
    "def init_epoch():\n",
    "    global epoch_cnt\n",
    "    epoch_cnt = 0\n",
    "\n",
    "# 모든 Log를 초기화\n",
    "def init_log():\n",
    "    global log_stack, iter_log, tloss_log, tacc_log, vloss_log, vacc_log, time_log\n",
    "    plt.rc('font', size = 10)\n",
    "    iter_log, tloss_log, tacc_log, vloss_log, vacc_log = [], [], [], [], []\n",
    "    time_log, log_stack = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_train_log(_tloss, _tacc, _time):\n",
    "    # Train Log 기록\n",
    "    time_log.append(_time)\n",
    "    tloss_log.append(_tloss)\n",
    "    tacc_log.append(_tacc)\n",
    "    iter_log.append(epoch_cnt)\n",
    "\n",
    "def record_valid_log(_vloss, _vacc):\n",
    "    # Validation Log 기록\n",
    "    vloss_log.append(_vloss)\n",
    "    vacc_log.append(_vacc)\n",
    "\n",
    "def last(log_list):\n",
    "    # last 안의 마지막 숫자를 반환(print_log 함수에서 사용)\n",
    "    if len(log_list) > 0:\n",
    "        return log_list[len(log_list) - 1]\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def print_log():\n",
    "    # 학습 추이 출력 : 소숫점 3자리까지\n",
    "    train_loss = round(float(last(tloss_log)), 3)\n",
    "    train_acc = round(float(last(tacc_log)), 3)\n",
    "    val_loss = round(float(last(vloss_log)), 3)\n",
    "    val_acc = round(float(last(vacc_log)), 3)\n",
    "    time_spent = round(float(last(time_log)), 3)\n",
    "\n",
    "    log_str = 'Epoch: {:3} | T_Loss {:5} | T_Acc {:5} | V_Loss {:5} | V_Acc {:5} | {:5}'.format(last(iter_log), train_loss, train_acc, val_loss, val_acc, time_spent)\n",
    "\n",
    "    log_stack.append(log_str)\n",
    "    \n",
    "    # 학습 추이 그래프 출력\n",
    "    hist_fig, loss_axis = plt.subplots(figsize=(10, 3), dpi=99)\n",
    "    hist_fig.patch.set_facecolor('white')\n",
    "\n",
    "    # Loss Line 구성\n",
    "    loss_t_line = plt.plot(iter_log, tloss_log, label='Train_Loss', color='red', marker='o')\n",
    "    loss_v_line = plt.plot(iter_log, vloss_log, label='Valid_Loss', color='blue', marker='s')\n",
    "    loss_axis.set_xlabel('epoch')\n",
    "    loss_axis.set_ylabel('loss')\n",
    "\n",
    "    # Acc, Line 구성\n",
    "    acc_axis = loss_axis.twinx()\n",
    "    acc_t_line = acc_axis.plot(iter_log, tacc_log, label='Train_Acc', color='red', marker='+')\n",
    "    acc_v_line = acc_axis.plot(iter_log, vacc_log, label='Valid_Acc', color='blue', marker='x')\n",
    "    acc_axis.set_ylabel('accuracy')\n",
    "\n",
    "    # 그래프 출력\n",
    "    hist_lines = loss_t_line + loss_v_line + acc_t_line + acc_v_line\n",
    "    loss_axis.legend(hist_lines, [l.get_label() for l in hist_lines])\n",
    "    loss_axis.grid()\n",
    "    plt.title('Learning history until epoch {}'.format(last(iter_log)))\n",
    "    plt.draw()\n",
    "\n",
    "    # 텍스트 로그 출력\n",
    "    clear_output(wait=True)\n",
    "    plt.show()\n",
    "    for idx in reversed(range(len(log_stack))):\n",
    "        print(log_stack[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    if device != 'cpu':\n",
    "        empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# 학습 알고리즘\n",
    "def epoch(data_loader, mode = 'train'):\n",
    "    global epoch_cnt\n",
    "\n",
    "    # 사용되는 변수 초기화\n",
    "    iter_loss, iter_acc, last_grad_performed = [], [], False\n",
    "\n",
    "    # 1 iteration 학습 알고리즘(for문을 나오면 1 epoch 완료)\n",
    "    for _data, _label in data_loader:\n",
    "        data, label = _data.to(device), _label.type(torch.LongTensor).to(device)\n",
    "\n",
    "        # 1. Feed-forward\n",
    "        if mode == 'train':\n",
    "            net.train()\n",
    "        else:\n",
    "            # 학습때만 쓰이는 Dropout, Batch Mormalization을 미사용\n",
    "            net.eval()\n",
    "\n",
    "        result = net(data) # 1 Batch에 대한 결과가 모든 Class에 대한 확률값으로\n",
    "        _, out = torch.max(result, 1) # result에서 최대 확률값을 기준으로 예측 class 도출( _ : 값 부분은 필요 없음, out : index 중 가장 큰 하나의 데이터)\n",
    "\n",
    "        # 2. Loss 계산\n",
    "        loss = loss_fn(result, label) # GT 와 Label 비교하여 Loss 산정\n",
    "        iter_loss.append(loss.item()) # 학습 추이를 위하여 Loss를 기록\n",
    "\n",
    "        # 3. 역전파 학습 후 Gradient Descent\n",
    "        if mode == 'train':\n",
    "            optim.zero_grad() # 미분을 통해 얻은 기울기를 초기화 for 다음 epoch\n",
    "            loss.backward() # 역전파 학습\n",
    "            optim.step() # Gradient Descent 수행\n",
    "            last_grad_performed = True # for문을 나가면 epoch 카운터 += 1\n",
    "\n",
    "        # 4. 정확도 계산\n",
    "        acc_partial = (out == label).float().sum() # GT == Label 인 개수\n",
    "        acc_partial = acc_partial / len(label) # ( TP / (TP + TM)) 해서 정확도 산출\n",
    "        iter_acc.append(acc_partial.item()) # 학습 추이를 위하여 Acc. 기록\n",
    "\n",
    "    # 역전파 학습 후 Epoch 카운터 += 1\n",
    "    if last_grad_performed:\n",
    "        epoch_cnt += 1\n",
    "\n",
    "    clear_memory()\n",
    "\n",
    "    # loss와 acc의 평균값 for 학습추이 그래프, 모든 GT와 Label 값 for 컨퓨전 매트릭스\n",
    "    return np.average(iter_loss), np.average(iter_acc)\n",
    "\n",
    "def epoch_not_finished():\n",
    "    # 에폭이 끝남을 알림\n",
    "    return epoch_cnt < maximum_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training initialization\n",
    "init_model()\n",
    "init_epoch()\n",
    "init_log()\n",
    "maximum_epoch = EPOCH\n",
    "\n",
    "# Training iteration\n",
    "\n",
    "while epoch_not_finished():\n",
    "    start_time = time.time()\n",
    "\n",
    "    tloss, tacc = epoch(train_loader, mode = 'train')\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    record_train_log(tloss, tacc, time_taken)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vloss, vacc = epoch(val_loader, mode = 'val')\n",
    "        record_valid_log(vloss, vacc)\n",
    "\n",
    "    print_log()\n",
    "\n",
    "print('\\n Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 검증\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc = epoch(test_loader, mode = 'test')\n",
    "    test_acc = round(test_acc, 4)\n",
    "    test_loss = round(test_loss, 4)\n",
    "    print('Test Acc.: {}'.format(test_acc))\n",
    "    print('Test Loss: {}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 resize 및 추출\n",
    "test_video_name = 'C_3_12_43_BU_SMC_10-14_12-17-14_CC_RGB_DF2_F2'\n",
    "test_video_path = f'/content/drive/MyDrive/Colab_Notebooks/Anomaly Detection/{test_video_name}.mp4'\n",
    "cv2.destroyAllWindows()\n",
    "cap = cv2.VideoCapture(test_video_path)\n",
    "img_list = []\n",
    "\n",
    "if cap.isOpened():\n",
    "\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if ret:\n",
    "            img = cv2.resize(img, (640, 640))\n",
    "            img_list.append(img)\n",
    "            # cv2_imshow(img)\n",
    "            # cv2.waitKey(1)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print('저장된 frame의 개수: {}'.format(len(img_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
